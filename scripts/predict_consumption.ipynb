{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '..'\n",
    "COUNTRY = 'malawi_2016'\n",
    "COUNTRY_DIR = os.path.join(BASE_DIR, 'countries', COUNTRY)\n",
    "PROCESSED_DIR = os.path.join(COUNTRY_DIR, 'processed')\n",
    "\n",
    "# these relate to the current country in question\n",
    "IMAGE_DIR = os.path.join(COUNTRY_DIR, 'images')\n",
    "RESULTS_DIR = os.path.join(COUNTRY_DIR, 'results')\n",
    "CNN_SAVE_FEATS_DIR = os.path.join(RESULTS_DIR, 'cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(CNN_SAVE_FEATS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extract with CNN\n",
    "If you have run this step before, you can skip it and run the commented out code in the next section to quick-start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(os.path.join(PROCESSED_DIR, 'image_download_locs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "def filename_to_im_tensor(file, transformer):\n",
    "    im = plt.imread(file)[:,:,:3]\n",
    "    im = transformer(im)\n",
    "    return im[None].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device} as backend')\n",
    "model = torch.load('trained_model.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rip off the final layers\n",
    "model.classifier = model.classifier[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ims = df_images.drop_duplicates(subset='image_names')\n",
    "unique_ims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.zeros((unique_ims.shape[0],4096))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 0\n",
    "batch_size = 4\n",
    "\n",
    "# this approach uses batching and should offer a speed-up over passing one image at a time by nearly 10x\n",
    "# runtime should be 5-7 minutes vs 45+ for a full forward pass\n",
    "while i + batch_size < len(ims):\n",
    "    ims_as_tensors = torch.cat([filename_to_im_tensor(os.path.join(IMAGE_DIR, ims[i+j])) for j in range(batch_size)], 0)\n",
    "    feats[i:i+batch_size,:] = model_ft(ims_as_tensors).cpu().detach().numpy()\n",
    "    i += batch_size\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=', ')\n",
    "\n",
    "# does the final batch of remaining images\n",
    "if len(ims) - i != 0:\n",
    "    rem = len(ims) - i\n",
    "    ims_as_tensors = torch.cat([filename_to_im_tensor(os.path.join(IMAGE_DIR, ims[i+j])) for j in range(rem)], 0)\n",
    "    feats[i:i+rem,:] = model_ft(ims_as_tensors).cpu().detach().numpy()\n",
    "    i += rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(CNN_SAVE_FEATS_DIR, 'forward_feats.npy'), feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ims = unique_ims[['image_names']]\n",
    "# this will be joined with the main df to show what index you should be looking at in feats\n",
    "unique_ims['feat_index'] = np.arange(len(unique_ims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption = pd.merge(left=df_images, right=unique_ims, on='image_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df_consumption.groupby(['clust_lat', 'clust_lon'])\n",
    "num_clusts = len(group)\n",
    "num_clusts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((num_clusts, 4096))\n",
    "y = []\n",
    "clusters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this goes through each cluster group and finds all images that are in the cluster\n",
    "# it aggregates the features for those images across the cluster\n",
    "for i, g in enumerate(group):\n",
    "    lat, long = g[0]\n",
    "    im_sub = df_consumption[(df_consumption['clust_lat'] == lat) & (df_consumption['clust_lon'] == long)].reset_index(drop=True)\n",
    "    agg_feats = np.zeros((len(im_sub), 4096))\n",
    "    for j, d in im_sub.iterrows():\n",
    "        agg_feats[j,:] = feats[d.feat_index]\n",
    "    agg_feats = agg_feats.mean(axis=0) # averages the features across all images in the cluster\n",
    "    \n",
    "    x[i,:] = agg_feats\n",
    "    y.append(g[1]['consumption'].values[0])\n",
    "    clusters.append([lat, long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(CNN_SAVE_FEATS_DIR, 'cluster_feats.npy'), x)\n",
    "pickle.dump(clusters, open(os.path.join(CNN_SAVE_FEATS_DIR, 'cluster_order.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.load(os.path.join(CNN_SAVE_FEATS_DIR, 'cluster_feats.npy'))\n",
    "# clusters = pickle.open(open(os.path.join(CNN_SAVE_FEATS_DIR, 'cluster_order.pkl'), 'rb'))\n",
    "# df_clusters = pd.read_csv(os.path.join(PROCESSED_DIR, 'clusters.csv'))\n",
    "# lookup = df_clusters.set_index(['lat', 'lon'])\n",
    "# y = []\n",
    "# for lat, lon = clusters:\n",
    "#     assert lat, lon in lookup\n",
    "#     y.append(lookup.loc[lat, lon]['cons_pc'])\n",
    "# y = np.array(y)\n",
    "# y_log = np.log(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bunch of code from the Jean et al Github that is modified to work with Python3 and our data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.linear_model as linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EllipseCollection\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def predict_consumption(\n",
    "    X, y, k=5, k_inner=5, points=10,\n",
    "        alpha_low=1, alpha_high=5, margin=0.25):\n",
    "    \"\"\"\n",
    "    Plots predicted consumption\n",
    "    \"\"\"\n",
    "    y_hat, r2 = run_cv(X, y, k, k_inner, points, alpha_low, alpha_high)\n",
    "    return X, y, y_hat, r2\n",
    "\n",
    "\n",
    "def run_cv(X, y, k, k_inner, points, alpha_low, alpha_high, randomize=False):\n",
    "    \"\"\"\n",
    "    Runs nested cross-validation to make predictions and compute r-squared.\n",
    "    \"\"\"\n",
    "    alphas = np.logspace(alpha_low, alpha_high, points)\n",
    "    r2s = np.zeros((k,))\n",
    "    y_hat = np.zeros_like(y)\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    fold = 0\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        r2s, y_hat, fold = evaluate_fold(\n",
    "            X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "            randomize)\n",
    "    return y_hat, r2s.mean()\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales features using StandardScaler.\n",
    "    \"\"\"\n",
    "    X_scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    X_train = X_scaler.fit_transform(X_train)\n",
    "    X_test = X_scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def train_and_predict_ridge(alpha, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Trains ridge model and predicts test set.\n",
    "    \"\"\"\n",
    "    ridge = linear_model.Ridge(alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_hat = ridge.predict(X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def predict_inner_test_fold(X, y, y_hat, train_idx, test_idx, alpha):\n",
    "    \"\"\"\n",
    "    Predicts inner test fold.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_hat[test_idx] = train_and_predict_ridge(alpha, X_train, y_train, X_test)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def find_best_alpha(X, y, k_inner, alphas):\n",
    "    \"\"\"\n",
    "    Finds the best alpha in an inner CV loop.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k_inner, shuffle=True)\n",
    "    best_alpha = 0\n",
    "    best_r2 = 0\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        y_hat = np.zeros_like(y)\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            y_hat = predict_inner_test_fold(\n",
    "                X, y, y_hat, train_idx, test_idx, alpha)\n",
    "        r2 = stats.pearsonr(y, y_hat)[0] ** 2\n",
    "        if r2 > best_r2:\n",
    "            best_alpha = alpha\n",
    "            best_r2 = r2\n",
    "    print('best alpha', best_alpha)\n",
    "    return best_alpha\n",
    "\n",
    "\n",
    "def evaluate_fold(\n",
    "    X, y, train_idx, test_idx, k_inner, alphas, r2s, y_hat, fold,\n",
    "        randomize):\n",
    "    \"\"\"\n",
    "    Evaluates one fold of outer CV.\n",
    "    \"\"\"\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    if randomize:\n",
    "        random.shuffle(y_train)\n",
    "    best_alpha = find_best_alpha(X_train, y_train, k_inner, alphas)\n",
    "    X_train, X_test = scale_features(X_train, X_test)\n",
    "    y_test_hat = train_and_predict_ridge(best_alpha, X_train, y_train, X_test)\n",
    "    r2 = stats.pearsonr(y_test, y_test_hat)[0] ** 2\n",
    "    r2s[fold] = r2\n",
    "    y_hat[test_idx] = y_test_hat\n",
    "    return r2s, y_hat, fold + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y_log = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, y_hat_log, r2 = predict_consumption(x, y_log)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, y_hat, r2 = predict_consumption(x, y)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.7*(len(x))) # let's use 70% of the data for training\n",
    "inds = np.arange(len(x))\n",
    "train_ind = np.random.choice(inds, n_train, replace=False)\n",
    "valid_ind = np.delete(inds, train_ind)\n",
    "\n",
    "train_x = x[train_ind]\n",
    "valid_x = x[valid_ind]\n",
    "\n",
    "train_y = y_log[train_ind]\n",
    "valid_y = y_log[valid_ind]\n",
    "\n",
    "ss = StandardScaler() # standardize features\n",
    "train_x = ss.fit_transform(train_x)\n",
    "valid_x = ss.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge(alpha=70) # the best alphas printed suggest using a high alphas\n",
    "ridge.fit(train_x, train_y)\n",
    "ridge.score(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y_hat_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(valid_y, preds, alpha=0.6)\n",
    "plt.plot(np.unique(valid_y), np.poly1d(np.polyfit(valid_y, preds, 1))(np.unique(valid_y)), color='g')\n",
    "plt.text(2.4, 1.5, 'r^2=0.43', size=12)\n",
    "plt.xlabel('Actual Log Consumption($/day)')\n",
    "plt.ylabel('Predicted Log Consumption($/day)')\n",
    "plt.title('Malawi Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predicting-poverty-replication",
   "language": "python",
   "name": "predicting-poverty-replication"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
